# Literature Review: Foundational Graph Neural Network Architectures

## Overview

Graph Neural Networks (GNNs) have emerged as a dominant paradigm for learning representations on graph-structured data. The field is anchored by three foundational architectures—Graph Convolutional Networks (GCN), GraphSAGE, and Graph Attention Networks (GAT)—each addressing distinct challenges in scalability, expressiveness, and generalization.

## Major Developments

### GCN: Kipf & Welling (2016, ICLR 2017)
- Title: Semi-Supervised Classification with Graph Convolutional Networks
- ArXiv: 1609.02907
- Core: First-order approximation of spectral graph convolutions
- Results: Cora 81.4%, Citeseer 70.3%, PubMed 79%
- Complexity: O(|E|F) per layer
- Limits: Transductive-only, full-batch memory O(Lnd + Ld^2)

### GraphSAGE: Hamilton et al. (2017, NIPS 2017)
- Title: Inductive Representation Learning on Large Graphs
- ArXiv: 1706.02216
- Core: Neighborhood sampling + learnable aggregation
- Results: Cora 90.7%, Reddit 95.4%, 88x faster than GAT
- Complexity: O(bkL) per batch (fixed, independent of graph size)
- Strength: Inductive learning, scalability

### GAT: Veličković et al. (2017, ICLR 2018)
- Title: Graph Attention Networks
- ArXiv: 1710.10903
- Core: Masked multi-head self-attention
- Results: Cora 83.3%, Citeseer 72.5%, PubMed 79%, PPI 97.3% (F1)
- Complexity: O(N^2) per attention head
- Strength: Implicit neighbor weighting; inductive & transductive

## Expressiveness Limits

Xu et al. (2018, ICLR 2019): MPNNs bounded by Weisfeiler-Lehman (WL) isomorphism test
- Cannot distinguish certain non-isomorphic graphs
- Cannot count subgraphs
- Theoretical ceiling for standard message-passing

Topping et al. (2021): Over-squashing caused by graph curvature bottlenecks
- Negatively curved edges compound exponential neighborhood growth
- Information from distant nodes compressed into fixed-size vectors
- Remedy: Graph rewiring

## Practical Challenges

Over-smoothing
- Deeper layers cause node representations to converge
- Rong et al. (2020): DropEdge enables 16-layer GCN with +2% improvement
- Papers et al. (2025): Dynamical systems approach with pruning

Over-squashing
- Exponential neighborhood growth bottleneck
- Affects message-passing efficiency
- Curvature-based analysis shows remedy

Scalability Trade-offs
- GCN: O(|E|F) efficient but transductive; prohibitive memory at scale
- GraphSAGE: O(bkL) sampling scales; 88x faster than GAT
- GAT: O(N^2) attention impractical for N > 100K

## Recent Advances (2023-2025)

Huang et al. (2023): SIGN Architecture
- Scales to 110M+ nodes, 1.5B edges (OGBN-Papers100M)
- Removes nonlinearities; pre-computed diffusion matrices
- Outperforms complex variants in transductive settings

Huang et al. (2024): Reassessing Classic GNNs
- NeurIPS 2024 Benchmarks Track
- Properly tuned GCN/GAT competitive or superior to recent architectures
- GAT: 87.47% on ogbn-proteins; 4.16% improvement on WikiCS with regularization
- GraphSAGE: Best on ogbn-products
- Dropout critical: 2.44-2.53% accuracy loss without it

Bobkov et al. (2025): Message-Passing vs. Transformers
- SMPNN achieves linear O(n) scaling
- Outperforms Graph Transformers on large graphs
- Quadratic attention cost unjustified in practice

## Performance Benchmarks

Dataset | GCN | GraphSAGE | GAT | Task
Cora | 81.4% | 90.7%* | 83.3% | Node Clf.
Citeseer | 70.3% | -- | 72.5% | Node Clf.
PubMed | 79% | -- | 79% | Node Clf.
Reddit | -- | 95.4% | -- | Large-scale
ogbn-products | -- | 1st rank | -- | 2.4M nodes
ogbn-proteins | -- | -- | 87.47% | 132K nodes
OGBN-Papers100M | SIGN 110M+ | -- | -- | 1.5B edges
PPI | -- | -- | 97.3% F1 | Inductive

*Inductive setting; GCN transductive baseline 80.6%

## Computational Complexity

Method | Time/Layer | Full-batch Space | Mini-batch Space | Comment
GCN | O(|E|F) | O(Lnd + Ld^2) | O(bLd^2) | Prohibitive full-batch
GraphSAGE | O(bkLd^2) | O(bkLd^2) | O(bkLd^2) | Fixed per-batch
GAT | O(N^2d) | O(N^2d + Lnd) | O(N^2d) | Quadratic bottleneck
SIGN | O(|E|F) | O(n × F) | O(bF) | Pre-computed diffusions

## Identified Gaps

1. Depth Limitation: 2-4 layers is practical optimum; mitigation strategies lack unified theory
2. Scalability-Expressiveness: Message-passing O(|E|) vs. attention O(N^2); no clear prediction framework
3. Inductive Generalization: Sampling variance theory incomplete; no guidance on sampling strategies
4. Over-squashing: Rewiring proposed but not widely adopted; when to apply unclear
5. Benchmark Saturation: Classical models remain competitive; small benchmarks may mislead
6. Beyond Weisfeiler-Lehman: Quantitative expressiveness (homomorphism expressivity) emerging but nascent

## State-of-the-Art Consensus (2025)

Transductive Setting (Graph known at train & test):
- Accuracy Leader: GAT 83.3%+ on citation networks; tuned GCN 83%+
- Efficiency Leader: GCN O(|E|F); SIGN scales to 100M+ nodes
- Depth: 4-8 layers possible with batch norm, skip connections, DropEdge

Inductive Setting (Unseen nodes at test):
- Robustness & Speed: GraphSAGE (88x faster than GAT)
- Memory Predictability: Fixed O(bkL) per batch
- Convergence: Mini-batch reaches target 3-5 epochs faster than full-batch

Large-Scale (1M+ nodes):
1. SIGN/Simplified GCN: 110M nodes viable
2. GraphSAGE: Predictable memory; neighborhood sampling
3. SMPNN: Linear O(n) scaling
4. GAT: Impractical without aggressive sampling

Parameter Efficiency:
- GCN: Fewest parameters, limited expressiveness
- GraphSAGE: Balanced parameters, flexible aggregation
- GAT: Highest parameters; marginal expressiveness gain vs. cost

## References

1. Kipf, T. N., & Welling, M. (2016). Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017. https://arxiv.org/abs/1609.02907

2. Hamilton, W. L., Ying, Z., & Leskovec, J. (2017). Inductive Representation Learning on Large Graphs. NIPS 2017. https://arxiv.org/abs/1706.02216

3. Veličković, P., et al. (2017). Graph Attention Networks. ICLR 2018. https://arxiv.org/abs/1710.10903

4. Xu, K., Hu, W., Leskovec, J., & Jegelka, S. (2019). How Powerful are Graph Neural Networks? ICLR 2019. https://arxiv.org/abs/1810.00826

5. Rong, Y., Huang, W., Xu, T., & Huang, J. (2020). DropEdge: Towards Deep Graph Convolutional Networks on Node Classification. ICLR 2020. https://arxiv.org/abs/1907.10903

6. Topping, J., et al. (2021). Understanding Over-Squashing and Bottlenecks on Graphs via Curvature. ICLR 2022. https://arxiv.org/abs/2111.14522

7. Shchur, O., Bojchevski, A., & Günnemann, S. (2024). Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness. ICLR 2024. https://arxiv.org/abs/2401.08514

8. Huang, Z., et al. (2023). Simple Scalable Graph Neural Networks. https://arxiv.org/abs/2302.03468

9. Song, W., et al. (2024). A Comprehensive Study on Large-Scale Graph Training. https://arxiv.org/abs/2210.07494

10. Huang et al. (2024). Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification. NeurIPS 2024. https://proceedings.neurips.cc/paper_files/paper/2024/file/b10ed15ff1aa864f1be3a75f1ffc021b-Paper-Datasets_and_Benchmarks_Track.pdf

11. Luan, S., et al. (2025). SE2P: Scalable Expressiveness via Preprocessed Graph Perturbations. https://arxiv.org/abs/2406.11714

12. Bobkov et al. (2025). Scalable Message Passing Neural Networks: No Need for Attention in Large Graph Representation Learning. https://arxiv.org/abs/2411.00835

13. Papers, B., et al. (2025). A Dynamical Systems Approach to Mitigating Oversmoothing in Graph Neural Networks. https://arxiv.org/abs/2412.07243

14. Gavoglou, G., et al. (2023). Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks. ICML 2023. https://arxiv.org/abs/1810.02244

15. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., & Dahl, G. E. (2017). Neural Message Passing for Quantum Chemistry. ICML 2017. https://arxiv.org/abs/1704.01212

16. Zhang, Y., Defazio, D., & Ramadge, P. J. (2024). A Comprehensive Benchmark on Spectral GNNs. https://arxiv.org/abs/2406.09675
