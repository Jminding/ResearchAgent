# Theoretical Framework: Reinforcement Learning for Surface Code Quantum Error Correction

## 1. Surface Code Distance-3 [[9,1,3]] Structure

### 1.1 Code Definition

The distance-3 surface code is a stabilizer code with parameters [[n, k, d]] = [[9, 1, 3]], encoding:
- n = 9 physical qubits
- k = 1 logical qubit
- d = 3 code distance (corrects up to floor((d-1)/2) = 1 error)

### 1.2 Qubit Layout

Physical qubits arranged on a 3x3 grid with indices:

```
Data qubits: q_0, q_1, ..., q_8

Grid layout:
    q_0 --- q_1 --- q_2
     |       |       |
    q_3 --- q_4 --- q_5
     |       |       |
    q_6 --- q_7 --- q_8
```

Ancilla qubits for syndrome measurement:
- X-type (plaquette) ancillas: a_X^(1), a_X^(2), a_X^(3), a_X^(4)
- Z-type (vertex) ancillas: a_Z^(1), a_Z^(2), a_Z^(3), a_Z^(4)

### 1.3 Stabilizer Operators

The stabilizer group S is generated by 8 independent generators (n - k = 8).

**Z-stabilizers (vertex operators):**
```
S_Z^(1) = Z_0 Z_1 Z_3         (top-left vertex)
S_Z^(2) = Z_1 Z_2 Z_4         (top-right vertex, boundary-adjusted)
S_Z^(3) = Z_3 Z_4 Z_6 Z_7     (bottom-left, interior)
S_Z^(4) = Z_4 Z_5 Z_7 Z_8     (bottom-right vertex)
```

**X-stabilizers (plaquette operators):**
```
S_X^(1) = X_0 X_1 X_3 X_4     (top-left plaquette)
S_X^(2) = X_1 X_2 X_4 X_5     (top-right plaquette)
S_X^(3) = X_3 X_4 X_6 X_7     (bottom-left plaquette)
S_X^(4) = X_4 X_5 X_7 X_8     (bottom-right plaquette)
```

Note: For the rotated surface code variant (more common), the stabilizers follow a checkerboard pattern. The exact form depends on boundary conditions. The canonical form uses:

**Rotated Surface Code d=3 (preferred):**
```
Physical qubit positions (9 data qubits):
       d_1
      /   \
    d_0     d_2
      \   /   \
       d_3     d_4
      /   \   /   \
    d_5     d_6     d_7
      \   /
       d_8

X-stabilizers (4 total):
S_X^(1) = X_0 X_1 X_3
S_X^(2) = X_1 X_2 X_3 X_4
S_X^(3) = X_3 X_5 X_6
S_X^(4) = X_4 X_6 X_7

Z-stabilizers (4 total):
S_Z^(1) = Z_0 Z_3 Z_5
S_Z^(2) = Z_1 Z_3 Z_4
S_Z^(3) = Z_2 Z_4 Z_7
S_Z^(4) = Z_5 Z_6 Z_8
```

### 1.4 Logical Operators

**Logical Z operator (vertical chain):**
```
Z_L = Z_0 Z_3 Z_5  (or any homologically equivalent chain)
```

**Logical X operator (horizontal chain):**
```
X_L = X_0 X_1 X_2  (or any homologically equivalent chain)
```

Properties:
- [Z_L, S] = 0 for all stabilizers S
- [X_L, S] = 0 for all stabilizers S
- {X_L, Z_L} = 0 (anticommute)
- Z_L^2 = X_L^2 = I

### 1.5 Syndrome Extraction

**Definition:** The syndrome s is a binary vector s in {0,1}^(n-k) = {0,1}^8

For error E acting on the code:
```
s_i = 0  if [S_i, E] = 0  (stabilizer and error commute)
s_i = 1  if {S_i, E} = 0  (stabilizer and error anticommute)
```

**Syndrome extraction circuit (per stabilizer):**
```
For Z-stabilizer S_Z = Z_i Z_j Z_k Z_l:
1. Initialize ancilla |0>
2. Apply CNOT from each data qubit to ancilla
3. Measure ancilla in Z-basis

For X-stabilizer S_X = X_i X_j X_k X_l:
1. Initialize ancilla |+>
2. Apply CNOT from ancilla to each data qubit
3. Measure ancilla in X-basis
```

**Syndrome measurement operator:**
```
M_s: H^(tensor 9) -> {0,1}^8
M_s(rho) = (s_X^(1), s_X^(2), s_X^(3), s_X^(4), s_Z^(1), s_Z^(2), s_Z^(3), s_Z^(4))
```

---

## 2. Noise Models for Transmon Qubits

### 2.1 Pauli Channel Formalism

A general Pauli channel on a single qubit:
```
E(rho) = (1 - p_x - p_y - p_z) rho + p_x X rho X + p_y Y rho Y + p_z Z rho Z
```

where p_x, p_y, p_z >= 0 and p_x + p_y + p_z <= 1.

### 2.2 Depolarizing Channel

**Single-qubit depolarizing channel with error probability p:**
```
D_p(rho) = (1 - p) rho + (p/3)(X rho X + Y rho Y + Z rho Z)
```

Pauli error probabilities:
```
p_x = p_y = p_z = p/3
p_I = 1 - p
```

**Physical interpretation:** Equal probability of X, Y, Z errors; models isotropic noise.

**Two-qubit depolarizing channel:**
```
D_p^(2)(rho) = (1 - p) rho + (p/15) sum_{P in {I,X,Y,Z}^2 \ {II}} P rho P
```

### 2.3 Dephasing Channel (Pure Dephasing)

**Single-qubit dephasing channel with dephasing probability p_z:**
```
Z_p(rho) = (1 - p_z) rho + p_z Z rho Z
```

Pauli error probabilities:
```
p_x = p_y = 0
p_z = p_z
p_I = 1 - p_z
```

**Physical interpretation:** Models T_2 decay; no energy exchange with environment.

### 2.4 Transmon-Specific Noise Mapping

For superconducting transmon qubits, noise is characterized by:

**Relaxation time T_1:** Energy decay |1> -> |0>
**Dephasing time T_2:** Phase coherence decay (T_2 <= 2*T_1)

**Amplitude damping + dephasing model:**
```
E_transmon(rho) = E_AD(E_PD(rho))
```

where:
- E_AD: Amplitude damping (T_1 process)
- E_PD: Pure dephasing (T_phi process, where 1/T_2 = 1/(2*T_1) + 1/T_phi)

**Effective Pauli channel approximation (for small gate time t_g):**

Given gate time t_g, T_1, T_2:
```
p_relax = 1 - exp(-t_g / T_1)
p_deph = (1 - exp(-t_g / T_2)) / 2

Effective Pauli probabilities:
p_x approx p_relax / 4
p_y approx p_relax / 4
p_z approx p_deph - p_relax / 4
```

**Simplified model for simulation:**
```
p_phys = total physical error rate per gate
gamma = dephasing bias parameter in [0, 1]

p_x = p_y = p_phys * (1 - gamma) / 3
p_z = p_phys * (1 + 2*gamma) / 3
```

- gamma = 0: Depolarizing (symmetric)
- gamma = 1: Pure dephasing (Z-biased)

### 2.5 Error Model on Surface Code

**Error locations:**
1. Data qubit idle errors (per syndrome cycle)
2. Gate errors (CNOT, single-qubit)
3. Measurement errors (readout)
4. State preparation errors

**Simplified circuit-level noise model:**
```
For each syndrome extraction cycle:
  1. Apply single-qubit errors to all data qubits: E_1q(p_idle)
  2. For each CNOT in syndrome circuit:
     - Apply CNOT
     - Apply two-qubit depolarizing error: D_p_2q
  3. Apply measurement error with probability p_meas
  4. Apply reset error with probability p_reset
```

**Phenomenological noise model (simplified):**
```
For each syndrome round r = 1, ..., R:
  1. Apply i.i.d. Pauli errors to each data qubit with probability p
  2. Extract syndrome with measurement error probability q
```

---

## 3. Reinforcement Learning Agent Architecture

### 3.1 Markov Decision Process (MDP) Formulation

**MDP tuple:** M = (S, A, T, R, gamma)

**State space S:**
```
s_t = (sigma_t, sigma_{t-1}, ..., sigma_{t-W+1}) in {0,1}^(8*W)
```
where:
- sigma_t in {0,1}^8 is the syndrome at time step t
- W is the history window length (hyperparameter)
- For d=3 surface code with single syndrome: sigma in {0,1}^8

**Extended state representation (optional):**
```
s_t = (sigma_t, delta_t, h_t)
```
where:
- delta_t = sigma_t XOR sigma_{t-1} (syndrome difference)
- h_t in Z^8: accumulated syndrome parity since last correction

**Action space A:**
```
A = P_n = {I, X, Y, Z}^n \ {I^n}
```

For computational tractability with n=9 qubits:
- Full action space: |A| = 4^9 - 1 = 262,143

**Reduced action space (recommended):**
```
A_reduced = {(i, P) : i in {0,...,8}, P in {X, Y, Z}}
```
Single-qubit corrections only: |A_reduced| = 27

**Correction chain action space:**
```
A_chain = {error chains matching syndrome s}
```
This requires precomputing or learning valid corrections.

**Transition dynamics T:**
```
T(s' | s, a) = Pr(sigma' | sigma, correction a, noise model)
```

For phenomenological noise:
```
T(s' | s, a) = sum_{E} Pr(E) * delta(s' = syndrome(a * E * current_state))
```

**Reward function R:**
```
R(s, a, s') = {
  +1   if correction successful (logical state preserved)
  -1   if logical error occurred
  0    otherwise (intermediate step)
}
```

**Alternative reward (per-step):**
```
R(s, a) = -lambda * |a| + beta * I[syndrome cleared]
```
where |a| = weight of correction (number of non-identity Paulis).

**Discount factor:** gamma in [0.99, 0.999] for episodic tasks

### 3.2 Neural Network Architecture

**Input layer:**
```
Input: s in {0,1}^(8*W)  (flattened syndrome history)
Embedding: Optional learned embedding of syndrome patterns
```

**Architecture options:**

**(A) Fully Connected Network (FCN):**
```
Layer 1: Linear(8*W, 128) + ReLU
Layer 2: Linear(128, 128) + ReLU
Layer 3: Linear(128, 64) + ReLU
Output:  Linear(64, |A|)  (Q-values or policy logits)
```

**(B) Graph Neural Network (GNN):**
```
Nodes: 8 stabilizers + 9 data qubits
Edges: Stabilizer-qubit connections from Tanner graph
Message passing: 3-4 layers
Aggregation: Mean/attention pooling
Output: Per-qubit correction probabilities
```

**(C) Transformer-based (for syndrome sequences):**
```
Input: Sequence of syndromes (sigma_1, ..., sigma_T)
Positional encoding
Multi-head self-attention layers
Output: Correction action distribution
```

**Recommended architecture for d=3:**
```
Encoder: FCN or small GNN
Input dim: 8 * W (syndrome bits * history window)
Hidden dims: [64, 64, 32]
Output dim: |A_reduced| = 27 (for single-qubit actions)
Activation: ReLU (hidden), Softmax (policy) or none (Q-values)
```

### 3.3 RL Algorithm Selection

**Option 1: Deep Q-Network (DQN)**
```
Q(s, a; theta) : S x A -> R
Loss: L(theta) = E[(r + gamma * max_{a'} Q(s', a'; theta^-) - Q(s, a; theta))^2]
```

**Option 2: Proximal Policy Optimization (PPO) [Recommended]**
```
Policy: pi(a|s; theta)
Value:  V(s; phi)

Objective: L^{CLIP}(theta) = E[min(r_t(theta) * A_t, clip(r_t(theta), 1-eps, 1+eps) * A_t)]
where r_t(theta) = pi(a_t|s_t; theta) / pi(a_t|s_t; theta_old)
      A_t = advantage estimate (GAE)
```

**Option 3: Soft Actor-Critic (SAC)**
```
For continuous relaxation of action space or when exploration is critical.
```

### 3.4 State-Action Value Function

**Q-function definition:**
```
Q^pi(s, a) = E_pi[sum_{k=0}^{infty} gamma^k R_{t+k+1} | S_t = s, A_t = a]
```

**Optimal Q-function:**
```
Q^*(s, a) = max_pi Q^pi(s, a)
```

**Bellman optimality equation:**
```
Q^*(s, a) = E[R(s,a,s') + gamma * max_{a'} Q^*(s', a')]
```

---

## 4. Logical Error Rate Scaling Hypothesis

### 4.1 Formal Hypothesis Statement

**Hypothesis H1 (Threshold Theorem for RL Decoder):**

Let:
- p = physical error rate per qubit per syndrome cycle
- d = code distance
- P_L(p, d) = logical error rate per syndrome cycle
- p_th = threshold error rate

**Claim:** There exist constants p_th > 0 and alpha > 0 such that for all p < p_th:
```
P_L(p, d) ~ A(p) * exp(-alpha(p) * d)
```

where:
- A(p) is a prefactor depending on p
- alpha(p) > 0 for p < p_th
- alpha(p) -> 0 as p -> p_th

**Equivalent scaling form:**
```
P_L(p, d) ~ (p / p_th)^{d/2}  for p << p_th
```

### 4.2 Variables and Definitions

**Physical error rate p:**
```
p = Pr(single Pauli error on one qubit in one cycle)
For depolarizing: p = p_x + p_y + p_z
```

**Logical error rate P_L:**
```
P_L = Pr(logical operator flipped after one QEC cycle)
P_L = Pr(E * C is homologically nontrivial)
```
where E = actual error, C = correction applied by decoder.

**Code distance d:**
```
d = min{|E| : E is undetectable logical error}
d = minimum weight of nontrivial logical operator
```

**Threshold p_th:**
```
p_th = sup{p : lim_{d->infty} P_L(p, d) = 0}
```

### 4.3 Testable Predictions

**Prediction 1:** For p < p_th, plot of log(P_L) vs. d should be linear with negative slope.
```
log(P_L(d)) = log(A) - alpha * d + O(1/d)
Slope: -alpha(p) < 0
```

**Prediction 2:** The slope alpha(p) should satisfy:
```
alpha(p) = -log(p / p_th) / 2 + higher order terms
```

**Prediction 3:** Threshold comparison with MWPM:
```
p_th^{RL} >= p_th^{MWPM} approx 0.103  (phenomenological noise)
```

**Prediction 4:** Sub-threshold scaling coefficient:
```
For RL decoder: alpha_{RL}(p) >= alpha_{MWPM}(p) for p < p_th
```

### 4.4 Falsification Criteria

The hypothesis is **falsified** if any of the following are observed:

1. **No exponential suppression:** P_L(d) does not decrease exponentially with d for any p < 0.10.

2. **No threshold:** P_L(d) increases with d for all tested p values.

3. **RL underperforms baseline:** For the same p, alpha_{RL} < alpha_{MWPM} significantly.

4. **Non-monotonic behavior:** alpha(p) is not monotonically decreasing in p.

### 4.5 Experimental Confirmation Criteria

The hypothesis is **supported** if:

1. For p in {0.01, 0.03, 0.05, 0.07, 0.09}, the log-linear fit of P_L vs. d has R^2 > 0.95.

2. Threshold estimate p_th^{RL} in [0.09, 0.15] (consistent with theoretical bounds).

3. For p = 0.05 < p_th: P_L(d=5) < P_L(d=3) by at least factor of 3.

---

## 5. Simulation and Training Pipeline Pseudocode

### 5.1 Surface Code Simulator

```
ALGORITHM: SurfaceCodeSimulator

INPUT:
  - d: code distance (d = 3 for [[9,1,3]] code)
  - p: physical error rate
  - noise_model: "depolarizing" | "dephasing" | "biased"
  - gamma: dephasing bias (0 = depolarizing, 1 = pure dephasing)

INITIALIZE:
  1. n_data = d^2 = 9 data qubits
  2. n_stab = d^2 - 1 = 8 stabilizers
  3. Build stabilizer matrix H in F_2^{n_stab x 2*n_data}
     - H[i, j] = 1 if stabilizer i acts on qubit j with X
     - H[i, n+j] = 1 if stabilizer i acts on qubit j with Z
  4. Build logical operators:
     - X_L = [x_L, 0] in F_2^{2n}
     - Z_L = [0, z_L] in F_2^{2n}
  5. Initialize error state E = [0]^{2n} (no error)

FUNCTION apply_noise(E, p, noise_model, gamma):
  FOR each qubit i in {0, ..., n-1}:
    r = uniform_random(0, 1)
    IF noise_model == "depolarizing":
      p_x = p_y = p_z = p/3
    ELSE IF noise_model == "dephasing":
      p_x = p_y = 0
      p_z = p
    ELSE IF noise_model == "biased":
      p_x = p_y = p * (1 - gamma) / 3
      p_z = p * (1 + 2*gamma) / 3

    IF r < p_x:
      E[i] = E[i] XOR 1  (X error on qubit i)
    ELSE IF r < p_x + p_y:
      E[i] = E[i] XOR 1
      E[n+i] = E[n+i] XOR 1  (Y error)
    ELSE IF r < p_x + p_y + p_z:
      E[n+i] = E[n+i] XOR 1  (Z error)
  RETURN E

FUNCTION extract_syndrome(E, H):
  s_X = (H_X @ E[n:2n]) mod 2  (X stabilizers detect Z errors)
  s_Z = (H_Z @ E[0:n]) mod 2   (Z stabilizers detect X errors)
  RETURN concatenate(s_X, s_Z)

FUNCTION apply_correction(E, C):
  RETURN (E XOR C) mod 2

FUNCTION check_logical_error(E, X_L, Z_L):
  x_err = (E[0:n] @ z_L) mod 2  (X part of error dotted with Z_L support)
  z_err = (E[n:2n] @ x_L) mod 2
  RETURN (x_err, z_err)  (1 if logical X/Z error, 0 otherwise)

OUTPUT:
  - extract_syndrome function
  - apply_correction function
  - check_logical_error function
```

### 5.2 Environment Wrapper (Gym-style)

```
ALGORITHM: QECEnvironment

CLASS QECEnvironment:

  INITIALIZE(d, p, noise_model, gamma, T_max, history_window):
    self.sim = SurfaceCodeSimulator(d, p, noise_model, gamma)
    self.T_max = T_max  (max steps per episode)
    self.W = history_window
    self.syndrome_dim = 2 * (d^2 - 1)  (8 for d=3)
    self.action_dim = 3 * d^2  (27 for d=3, single-qubit corrections)
    CALL self.reset()

  FUNCTION reset():
    self.E = [0]^{2n}  (no initial error)
    self.t = 0
    self.syndrome_history = deque(maxlen=W)
    FOR i in 1 to W:
      self.syndrome_history.append([0]^{syndrome_dim})
    initial_syndrome = self.sim.extract_syndrome(self.E)
    self.syndrome_history.append(initial_syndrome)
    RETURN self._get_state()

  FUNCTION _get_state():
    RETURN flatten(list(self.syndrome_history))  # shape: (W * syndrome_dim,)

  FUNCTION _action_to_correction(a):
    # a in {0, ..., 26} for single-qubit corrections
    qubit_idx = a // 3
    pauli_type = a mod 3  (0=X, 1=Y, 2=Z)
    C = [0]^{2n}
    IF pauli_type == 0:  # X
      C[qubit_idx] = 1
    ELSE IF pauli_type == 1:  # Y
      C[qubit_idx] = 1
      C[n + qubit_idx] = 1
    ELSE:  # Z
      C[n + qubit_idx] = 1
    RETURN C

  FUNCTION step(action):
    # Apply correction
    C = self._action_to_correction(action)
    self.E = self.sim.apply_correction(self.E, C)

    # Apply noise (new errors)
    self.E = self.sim.apply_noise(self.E, p, noise_model, gamma)

    # Extract new syndrome
    new_syndrome = self.sim.extract_syndrome(self.E)
    self.syndrome_history.append(new_syndrome)

    # Check for logical error
    x_err, z_err = self.sim.check_logical_error(self.E)
    logical_error = (x_err OR z_err)

    # Compute reward
    IF logical_error:
      reward = -1.0
      done = True
    ELSE IF self.t >= self.T_max:
      reward = +1.0  (survived full episode)
      done = True
    ELSE:
      reward = 0.0  (or small positive for syndrome reduction)
      done = False

    self.t += 1
    next_state = self._get_state()

    RETURN next_state, reward, done, {"logical_error": logical_error}

  FUNCTION get_action_space():
    RETURN Discrete(self.action_dim)

  FUNCTION get_observation_space():
    RETURN Box(low=0, high=1, shape=(W * syndrome_dim,), dtype=int)
```

### 5.3 RL Agent (PPO)

```
ALGORITHM: PPOAgent

HYPERPARAMETERS:
  lr_actor = 3e-4       # Actor learning rate
  lr_critic = 1e-3      # Critic learning rate
  gamma = 0.99          # Discount factor
  lambda_gae = 0.95     # GAE parameter
  eps_clip = 0.2        # PPO clipping parameter
  K_epochs = 10         # PPO update epochs
  batch_size = 64
  buffer_size = 2048    # Steps before update

CLASS PPOAgent:

  INITIALIZE(state_dim, action_dim, hidden_dims=[64, 64]):
    # Actor network (policy)
    self.actor = MLP(
      input_dim = state_dim,
      hidden_dims = hidden_dims,
      output_dim = action_dim,
      output_activation = "softmax"
    )

    # Critic network (value function)
    self.critic = MLP(
      input_dim = state_dim,
      hidden_dims = hidden_dims,
      output_dim = 1,
      output_activation = None
    )

    self.optimizer_actor = Adam(self.actor.parameters(), lr=lr_actor)
    self.optimizer_critic = Adam(self.critic.parameters(), lr=lr_critic)
    self.buffer = RolloutBuffer(buffer_size)

  FUNCTION select_action(state, deterministic=False):
    probs = self.actor(state)
    IF deterministic:
      action = argmax(probs)
    ELSE:
      action = sample_categorical(probs)
    log_prob = log(probs[action])
    RETURN action, log_prob

  FUNCTION compute_gae(rewards, values, dones):
    advantages = []
    gae = 0
    FOR t in reverse(range(len(rewards))):
      IF t == len(rewards) - 1:
        next_value = 0
      ELSE:
        next_value = values[t+1]
      delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
      gae = delta + gamma * lambda_gae * (1 - dones[t]) * gae
      advantages.insert(0, gae)
    returns = advantages + values
    RETURN advantages, returns

  FUNCTION update():
    states, actions, old_log_probs, rewards, dones = self.buffer.get()

    # Compute values and advantages
    values = self.critic(states).squeeze()
    advantages, returns = self.compute_gae(rewards, values, dones)
    advantages = normalize(advantages)

    FOR epoch in 1 to K_epochs:
      FOR batch in get_batches(states, actions, old_log_probs, advantages, returns, batch_size):
        s_batch, a_batch, old_lp_batch, adv_batch, ret_batch = batch

        # Actor loss (PPO-clip)
        probs = self.actor(s_batch)
        new_log_probs = log(probs[a_batch])
        ratio = exp(new_log_probs - old_lp_batch)
        surr1 = ratio * adv_batch
        surr2 = clip(ratio, 1-eps_clip, 1+eps_clip) * adv_batch
        actor_loss = -mean(min(surr1, surr2))

        # Critic loss (MSE)
        new_values = self.critic(s_batch).squeeze()
        critic_loss = mean((new_values - ret_batch)^2)

        # Update networks
        self.optimizer_actor.zero_grad()
        actor_loss.backward()
        self.optimizer_actor.step()

        self.optimizer_critic.zero_grad()
        critic_loss.backward()
        self.optimizer_critic.step()

    self.buffer.clear()
    RETURN actor_loss, critic_loss

  FUNCTION save(path):
    save_model(self.actor, path + "/actor.pt")
    save_model(self.critic, path + "/critic.pt")

  FUNCTION load(path):
    load_model(self.actor, path + "/actor.pt")
    load_model(self.critic, path + "/critic.pt")
```

### 5.4 Training Loop

```
ALGORITHM: TrainRLDecoder

INPUT:
  - code_distances: [3, 5, 7]  (list of distances to train on)
  - error_rates: [0.01, 0.03, 0.05, 0.07, 0.09, 0.11]
  - noise_model: "depolarizing" | "biased"
  - gamma_bias: 0.0 (depolarizing) or 0.5 (biased)
  - n_episodes: 100000
  - T_max: 100 (max steps per episode)
  - history_window: 3
  - eval_interval: 1000
  - n_eval_episodes: 1000

INITIALIZE:
  results = {}
  FOR d in code_distances:
    FOR p in error_rates:
      results[(d, p)] = {"train_rewards": [], "eval_P_L": []}

MAIN TRAINING LOOP:
  FOR d in code_distances:
    FOR p in error_rates:
      PRINT("Training d={d}, p={p}")

      # Create environment
      env = QECEnvironment(d, p, noise_model, gamma_bias, T_max, history_window)
      state_dim = env.get_observation_space().shape[0]
      action_dim = env.get_action_space().n

      # Create agent
      agent = PPOAgent(state_dim, action_dim, hidden_dims=[64, 64])

      # Training loop
      total_steps = 0
      FOR episode in 1 to n_episodes:
        state = env.reset()
        episode_reward = 0
        done = False

        WHILE NOT done:
          action, log_prob = agent.select_action(state)
          next_state, reward, done, info = env.step(action)

          # Store transition
          agent.buffer.add(state, action, log_prob, reward, done)

          state = next_state
          episode_reward += reward
          total_steps += 1

          # Update agent when buffer is full
          IF agent.buffer.is_full():
            actor_loss, critic_loss = agent.update()

        results[(d, p)]["train_rewards"].append(episode_reward)

        # Periodic evaluation
        IF episode mod eval_interval == 0:
          P_L = evaluate_agent(agent, env, n_eval_episodes)
          results[(d, p)]["eval_P_L"].append((episode, P_L))
          PRINT(f"Episode {episode}: P_L = {P_L:.4f}")

      # Save trained agent
      agent.save(f"models/agent_d{d}_p{p}")

FUNCTION evaluate_agent(agent, env, n_episodes):
  n_logical_errors = 0
  FOR i in 1 to n_episodes:
    state = env.reset()
    done = False
    WHILE NOT done:
      action, _ = agent.select_action(state, deterministic=True)
      state, reward, done, info = env.step(action)
    IF info["logical_error"]:
      n_logical_errors += 1
  RETURN n_logical_errors / n_episodes

OUTPUT: results dictionary, saved models
```

### 5.5 Analysis and Threshold Estimation

```
ALGORITHM: AnalyzeScaling

INPUT:
  - results: dictionary from training
  - code_distances: [3, 5, 7]
  - error_rates: [0.01, 0.03, 0.05, 0.07, 0.09, 0.11]

STEP 1: Extract logical error rates
  P_L_matrix = zeros(len(error_rates), len(code_distances))
  FOR i, p in enumerate(error_rates):
    FOR j, d in enumerate(code_distances):
      P_L_matrix[i, j] = results[(d, p)]["eval_P_L"][-1][1]  # final P_L

STEP 2: Fit exponential scaling for each p
  scaling_params = {}
  FOR i, p in enumerate(error_rates):
    P_L_values = P_L_matrix[i, :]

    # Fit: log(P_L) = log(A) - alpha * d
    # Using linear regression on log(P_L) vs d
    log_P_L = log(P_L_values + 1e-10)  # avoid log(0)
    slope, intercept, r_squared = linear_regression(code_distances, log_P_L)

    alpha = -slope
    A = exp(intercept)
    scaling_params[p] = {"alpha": alpha, "A": A, "R2": r_squared}

    PRINT(f"p={p}: alpha={alpha:.3f}, A={A:.4f}, R^2={r_squared:.4f}")

STEP 3: Estimate threshold
  # Threshold is where alpha -> 0
  # Fit alpha(p) and find p where alpha = 0
  p_values = list(scaling_params.keys())
  alpha_values = [scaling_params[p]["alpha"] for p in p_values]

  # Fit: alpha(p) = a * (p_th - p) for p < p_th
  # Linear fit: alpha = a * p_th - a * p
  a_fit, b_fit = linear_regression(p_values, alpha_values)
  p_th_estimate = -b_fit / a_fit

  PRINT(f"Estimated threshold: p_th = {p_th_estimate:.4f}")

STEP 4: Generate plots
  PLOT 1: P_L vs d (log-linear scale) for each p
    FOR p in error_rates:
      plot(code_distances, log(P_L[p, :]), label=f"p={p}")
    xlabel("Code distance d")
    ylabel("log(P_L)")
    title("Logical Error Rate Scaling")
    save("figures/P_L_vs_d.png")

  PLOT 2: alpha(p) vs p
    plot(p_values, alpha_values)
    axhline(y=0)
    axvline(x=p_th_estimate, linestyle='--')
    xlabel("Physical error rate p")
    ylabel("Scaling exponent alpha")
    title("Threshold Estimation")
    save("figures/alpha_vs_p.png")

  PLOT 3: P_L vs p for each d
    FOR d in code_distances:
      plot(error_rates, P_L[:, d], label=f"d={d}")
    axvline(x=p_th_estimate, linestyle='--')
    xlabel("Physical error rate p")
    ylabel("Logical error rate P_L")
    title("Logical Error Rate vs Physical Error Rate")
    save("figures/P_L_vs_p.png")

STEP 5: Compare with MWPM baseline
  # Run MWPM decoder on same scenarios
  P_L_mwpm = evaluate_mwpm_decoder(code_distances, error_rates, n_eval_episodes)

  FOR d in code_distances:
    plot(error_rates, P_L_rl[:, d], label=f"RL d={d}")
    plot(error_rates, P_L_mwpm[:, d], label=f"MWPM d={d}", linestyle='--')
  xlabel("Physical error rate p")
  ylabel("Logical error rate P_L")
  title("RL vs MWPM Decoder Comparison")
  save("figures/rl_vs_mwpm.png")

OUTPUT:
  - scaling_params dictionary
  - p_th_estimate
  - Comparison figures
```

---

## 6. Summary of Key Variables and Parameters

### 6.1 Code Parameters
| Symbol | Description | Value (d=3) |
|--------|-------------|-------------|
| n | Number of data qubits | 9 |
| k | Number of logical qubits | 1 |
| d | Code distance | 3 |
| n_stab | Number of stabilizers | 8 |

### 6.2 Noise Model Parameters
| Symbol | Description | Range |
|--------|-------------|-------|
| p | Physical error rate | [0.001, 0.15] |
| gamma | Dephasing bias | [0, 1] |
| p_x, p_y, p_z | Individual Pauli error rates | [0, p] |
| p_meas | Measurement error rate | [0, 0.1] |

### 6.3 RL Hyperparameters
| Symbol | Description | Recommended |
|--------|-------------|-------------|
| W | Syndrome history window | 3-5 |
| lr_actor | Actor learning rate | 3e-4 |
| lr_critic | Critic learning rate | 1e-3 |
| gamma | Discount factor | 0.99 |
| lambda_gae | GAE parameter | 0.95 |
| eps_clip | PPO clip parameter | 0.2 |
| K_epochs | PPO update epochs | 10 |
| batch_size | Mini-batch size | 64 |
| hidden_dims | MLP hidden dimensions | [64, 64] |

### 6.4 Experimental Parameters
| Symbol | Description | Recommended |
|--------|-------------|-------------|
| n_episodes | Training episodes | 100,000 |
| T_max | Max steps per episode | 100 |
| n_eval | Evaluation episodes | 1,000-10,000 |
| eval_interval | Evaluation frequency | 1,000 episodes |

---

## 7. Assumptions and Limitations

### 7.1 Model Assumptions

1. **Independent errors:** Errors on different qubits are statistically independent.

2. **Markovian noise:** No memory effects in noise; errors depend only on current state.

3. **Perfect syndrome extraction:** In phenomenological model, syndrome measurement is noiseless (or treated separately with p_meas).

4. **Pauli error model:** All errors can be represented as Pauli operators (no coherent errors).

5. **Static code:** Code structure does not change during operation.

### 7.2 RL Assumptions

1. **Full observability:** Agent has access to complete syndrome information.

2. **Discrete actions:** Corrections are discrete Pauli operators.

3. **Stationary policy:** Optimal policy does not depend on absolute time.

4. **Sufficient capacity:** Neural network can represent optimal decoder.

### 7.3 Known Limitations

1. **Scalability:** Action space grows exponentially with n; may need hierarchical/factored approaches for large d.

2. **Sample efficiency:** RL may require many samples; consider offline RL or imitation learning from MWPM.

3. **Generalization:** Agent trained at one p may not generalize to other error rates.

4. **Circuit-level noise:** Phenomenological model is simplified; circuit-level simulation is more accurate but computationally expensive.

---

## 8. Appendix: Mathematical Definitions

### 8.1 Pauli Group
```
P_1 = {+/- I, +/- iI, +/- X, +/- iX, +/- Y, +/- iY, +/- Z, +/- iZ}
P_n = P_1^{tensor n}
```

### 8.2 Stabilizer Code
A stabilizer code C is defined by an abelian subgroup S < P_n with -I not in S:
```
C = {|psi> : S|psi> = |psi> for all S in S}
```

### 8.3 Symplectic Representation
Pauli operator P = i^k X^a Z^b where a, b in {0,1}^n.
Binary representation: (a | b) in F_2^{2n}

Commutation: [P, Q] = 0 iff <(a_P, b_P), (a_Q, b_Q)>_symplectic = 0
where <u, v>_symplectic = u_x . v_z + u_z . v_x (mod 2)

### 8.4 Minimum Weight Perfect Matching (MWPM) Baseline
For comparison, the MWPM decoder:
1. Construct syndrome graph with nodes at defect locations
2. Add edges with weights = log(p / (1-p)) * distance
3. Find minimum weight perfect matching
4. Infer error chain from matching
5. Apply correction

Threshold for MWPM on surface code: p_th approximately equal to 0.103 (phenomenological), approximately equal to 0.007 (circuit-level depolarizing).
